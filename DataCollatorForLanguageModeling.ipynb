{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import DataCollatorForLanguageModeling, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data collator for language modeling\n",
    "- huggingface의 transformer 라이브러리에서 제공하는 class\n",
    "- 언어 모델링(특히 masked 언어 모델링) 작업을 위한 데이터 처리\n",
    "- masking, batch 처리 기능\n",
    "- parameters\n",
    "    - tokenizer: 토크나이저\n",
    "    - mlm: masking된 언어 모델링 작업을 수행할 지 여부, 기본값=True\n",
    "    - mlm_probability: 입력 토큰을 mask로 대체할 확률, 기본값=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer trained on Korean commerce data\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('tkcho/commercelanguage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mlm=True인 경우\n",
    "- label의 -100은 손실함수에서 무시됨(mask가 아닌것, padding이 -100으로 설정됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [{'input_ids': [101, 2000, 2022, 102], 'special_tokens_mask': [1, 0, 0, 1]}, {'input_ids': [101, 20, 30, 40, 102], 'special_tokens_mask': [1, 0, 0, 0, 1]}] \n",
      "\n",
      "output:  {'input_ids': tensor([[ 101,    4, 2022,  102,    0],\n",
      "        [ 101,   20,   30,   40,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]]), 'labels': tensor([[-100, 2000, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100]])} \n",
      "\n",
      "input_ids\n",
      " tensor([[ 101,    4, 2022,  102,    0],\n",
      "        [ 101,   20,   30,   40,  102]]) \n",
      "\n",
      "attention_mask\n",
      " tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]]) \n",
      "\n",
      "labels\n",
      " tensor([[-100, 2000, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=True, # masked languge model\n",
    "        mlm_probability=0.30 # masking probability\n",
    "    )\n",
    "\n",
    "examples = [{\"input_ids\": [101, 2000, 2022, 102], \"special_tokens_mask\": [1, 0, 0, 1]},\n",
    "           {\"input_ids\": [101, 20, 30, 40, 102], \"special_tokens_mask\": [1, 0, 0, 0, 1]}]\n",
    "print('input: ',examples,'\\n')\n",
    "\n",
    "batch = data_collator(examples)\n",
    "print('output: ',batch,'\\n')\n",
    "\n",
    "# labels에 -100이 아닌것 : masking된 token\n",
    "print('input_ids\\n', batch['input_ids'],'\\n')\n",
    "print('attention_mask\\n', batch['attention_mask'],'\\n')\n",
    "print('labels\\n', batch['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mlm=False 인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [{'input_ids': [101, 2000, 2022, 102], 'special_tokens_mask': [1, 0, 0, 1]}, {'input_ids': [101, 20, 30, 40, 102], 'special_tokens_mask': [1, 0, 0, 0, 1]}] \n",
      "\n",
      "output:  {'input_ids': tensor([[ 101, 2000, 2022,  102,    0],\n",
      "        [ 101,   20,   30,   40,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]]), 'labels': tensor([[ 101, 2000, 2022,  102, -100],\n",
      "        [ 101,   20,   30,   40,  102]])} \n",
      "\n",
      "input_ids\n",
      " tensor([[ 101, 2000, 2022,  102,    0],\n",
      "        [ 101,   20,   30,   40,  102]]) \n",
      "\n",
      "attention_mask\n",
      " tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]]) \n",
      "\n",
      "labels\n",
      " tensor([[ 101, 2000, 2022,  102, -100],\n",
      "        [ 101,   20,   30,   40,  102]])\n"
     ]
    }
   ],
   "source": [
    "data_collator_2 = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "examples = [{\"input_ids\": [101, 2000, 2022, 102], \"special_tokens_mask\": [1, 0, 0, 1]},\n",
    "           {\"input_ids\": [101, 20, 30, 40, 102], \"special_tokens_mask\": [1, 0, 0, 0, 1]}]\n",
    "print('input: ',examples,'\\n')\n",
    "\n",
    "batch = data_collator_2(examples)\n",
    "print('output: ',batch,'\\n')\n",
    "\n",
    "# labels이 -100 : padding\n",
    "print('input_ids\\n', batch['input_ids'],'\\n')\n",
    "print('attention_mask\\n', batch['attention_mask'],'\\n')\n",
    "print('labels\\n', batch['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test on Korean commerce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch, tokenizer=tokenizer):\n",
    "    '''\n",
    "    map 이용해서 dataset tokenizing 처리 하기 위함\n",
    "    '''\n",
    "    encoded = tokenizer(batch['text'], padding=True, truncation=True)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':['무인양품 best 오늘만 74% 세일','다우니 섬유유연제 패밀리팩','상하목장 유기농 락토프리 100개']})\n",
    "\n",
    "train_ds = Dataset.from_pandas(df[['text']])\n",
    "train_ds = DatasetDict({'train':train_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9ecd7041414140b1edadab685d4bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encoded = train_ds.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLS] 무인양품 best 오늘만 74 % 세일 [SEP]\n",
      "[CLS] 무인 [MASK]품 best 오늘만 74 % [MASK] [SEP]\n",
      "\n",
      "[CLS] 다우니 섬유유연제 패밀리팩 [SEP]\n",
      "[CLS] [MASK] 섬유유연제 [MASK]팩 [SEP]\n",
      "\n",
      "[CLS] 상하목장 유기농 락토프리 100개 [SEP]\n",
      "[CLS] [MASK] 유기농 [MASK] [MASK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "# masking sample\n",
    "import re\n",
    "\n",
    "for i in range(len(train_encoded['train'])):\n",
    "    print('')\n",
    "    samples = [train_encoded['train'][i]]\n",
    "    collated_samples = data_collator(samples)\n",
    "    masked_positions = (collated_samples['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1].tolist()\n",
    "    original_tokens = [tokenizer.decode(token_id) for token_id in collated_samples['labels'][0, masked_positions]]\n",
    "    \n",
    "    d = tokenizer.decode(collated_samples.input_ids[0])\n",
    "    o = tokenizer.decode(samples[0]['input_ids'])\n",
    "    print(re.findall('.+\\[SEP\\]',o)[0])\n",
    "    print(re.findall('.+\\[SEP\\]',d)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
